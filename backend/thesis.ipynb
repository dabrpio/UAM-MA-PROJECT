{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr/UAM-MS-PROJECT/backend/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PyTorch version 2.4.1 available.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from rapidfuzz import process, fuzz, utils\n",
    "from typing import Dict, Literal, TypedDict\n",
    "from tqdm import tqdm\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import subprocess\n",
    "import sacrebleu\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "\n",
    "def make_openai_api_call(context: str, prompt: str):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": context },\n",
    "            { \"role\": \"user\", \"content\": prompt }\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationExample(TypedDict):\n",
    "    text: str\n",
    "    line: str\n",
    "    confidence: float\n",
    "    translation: str\n",
    "\n",
    "def find_n_fuzzy_matches(\n",
    "        text: str, \n",
    "        source_filename: str, \n",
    "        n: int\n",
    "    ) -> list[tuple[str,float, int]]:\n",
    "    with open(source_filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        matches = process.extract(\n",
    "            query=text, \n",
    "            choices=lines, \n",
    "            scorer=fuzz.ratio, \n",
    "            limit=n, \n",
    "            processor=utils.default_process\n",
    "        )\n",
    "        \n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line_n(filename: str, n: int):\n",
    "    content = subprocess.run(\n",
    "        args=['sed', '-n', f'{n}p', filename], \n",
    "        capture_output=True, \n",
    "        text=True\n",
    "    )\n",
    "    return content.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind_fuzzy_matches_with_translations(\n",
    "    fuzzy_matches: list[tuple[str,float, int]], \n",
    "    target_filename: str\n",
    "    ) -> list[TranslationExample]:\n",
    "    result = []\n",
    "    for text, score, line in fuzzy_matches:\n",
    "        result.append(TranslationExample({\n",
    "            \"text\": text.strip(),\n",
    "            \"score\": round(score, 2),\n",
    "            \"translation\": read_line_n(\n",
    "                filename=target_filename, \n",
    "                n=line + 1\n",
    "            ).strip()\n",
    "        }))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Language = Literal[\"polski\",\"angielski\"]\n",
    "TEMP_FILENAME = \"./data/temp.txt\"\n",
    "context = \"Jesteś pomocnym bilingwalnym tłumaczem specjalizującym się w tłumaczeniach pomiędzy językiem polskim, a angielskim. Jako wynik zwracasz samo tłumaczenie.\"\n",
    "\n",
    "lang_to_filename: Dict[Language, str] = {\n",
    "    \"angielski\": \"./data/train.en.txt\",\n",
    "    \"polski\" : \"./data/train.pl.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(\n",
    "    text: str,\n",
    "    source_language: Language,\n",
    "    target_language: Language,\n",
    "    n_shots: int):\n",
    "    source_filename = lang_to_filename[source_language]\n",
    "    target_filename = lang_to_filename[target_language]\n",
    "\n",
    "    matches = find_n_fuzzy_matches(\n",
    "        text=text, \n",
    "        source_filename=source_filename, \n",
    "        n=n_shots\n",
    "    )\n",
    "\n",
    "    matches_with_translations = bind_fuzzy_matches_with_translations(\n",
    "        fuzzy_matches=matches,\n",
    "        target_filename=target_filename\n",
    "    )\n",
    "   \n",
    "    def create_shot(match: TranslationExample):\n",
    "        return f\"{source_language}: {match['text']}\\n\" + \\\n",
    "               f\"{target_language}: {match['translation']}\"\n",
    "\n",
    "    prompt = \\\n",
    "        f\"Przetłumacz zdania z języka {source_language}ego \" + \\\n",
    "        f\"na język {target_language}, biorąc pod uwagę \" + \\\n",
    "        f\"przykłady tłumaczeń zdań przybliżonych.\\n\" + \\\n",
    "        \"\\n\".join([create_shot(m) for m in matches_with_translations]) + \\\n",
    "        f\"\\n{source_language}: {text}\" + \\\n",
    "        f\"\\n{target_language}: \"\n",
    "    translation = make_openai_api_call(context, prompt)\n",
    "    \n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(text: str, source_language: Language, target_language: Language, n_shots: list[int]):\n",
    "    translations = []\n",
    "\n",
    "    if max(n_shots) > 1:\n",
    "        matches = find_n_fuzzy_matches(\n",
    "            text=text, \n",
    "            source_filename=lang_to_filename[source_language], \n",
    "            target_filename=lang_to_filename[target_language], \n",
    "            n=max(n_shots)\n",
    "        )\n",
    "    \n",
    "    for n in n_shots:\n",
    "        if n == 0:\n",
    "            prompt = f\"Przetłumacz z języka {source_language}ego na język {target_language}.\\n\" + \\\n",
    "                    f\"{source_language}: {text}\\n\" + \\\n",
    "                    f\"{target_language}:\"\n",
    "            t = make_openai_api_call(context, prompt)\n",
    "            translations.append(t)\n",
    "        else:\n",
    "            def create_shot(match: str):\n",
    "                return f\"{source_language}: {match.get('text')}\\n\" + \\\n",
    "                    f\"{target_language}: {match.get('translation')}\"\n",
    "\n",
    "            prompt = f\"Przetłumacz zdania z języka {source_language}ego\" + \\\n",
    "                    f\"na język {target_language}, biorąc pod uwagę \" + \\\n",
    "                    f\"przykłady tłumaczeń zdań podobnych.\\n\" + \\\n",
    "                    \"\\n\".join([create_shot(m) for m in matches[:n]]) + \\\n",
    "                    f\"\\n{source_language}: {text}\" + \\\n",
    "                    f\"\\n{target_language}: \"\n",
    "            t = make_openai_api_call(context, prompt)\n",
    "            translations.append(t)\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_lang_to_filename: Dict[Language, str] = {\n",
    "    \"angielski\": './data/val.en.txt',\n",
    "    \"polski\" : './data/val.pl.txt'\n",
    "}\n",
    "\n",
    "def ngram_translations(source_language: Language, target_langauge: Language, ngrams: list[str], target_folder: str):\n",
    "    source_file = val_lang_to_filename[source_language]\n",
    "\n",
    "    with open(source_file, \"r\") as file,\\\n",
    "         open(f\"{target_folder}/00.txt\", 'a') as file_res_zero,\\\n",
    "         open(f\"{target_folder}/02.txt\", 'a') as file_res_two,\\\n",
    "         open(f\"{target_folder}/05.txt\", 'a') as file_res_five,\\\n",
    "         open(f\"{target_folder}/10.txt\", 'a') as file_res_ten:\n",
    "        \n",
    "        for sentence in tqdm(file):\n",
    "            sentence = sentence.strip()\n",
    "            translations = translate_batch(sentence, source_language, target_langauge, ngrams)\n",
    "\n",
    "            file_res_zero.write(str(translations[0]).strip() + '\\n')\n",
    "            file_res_two.write(str(translations[1]).strip() + '\\n')\n",
    "            file_res_five.write(str(translations[2]).strip() + '\\n')\n",
    "            file_res_ten.write(str(translations[3]).strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [3:59:50, 14.39s/it]\n"
     ]
    }
   ],
   "source": [
    "ngram_translations('angielski', 'polski', [0, 2, 5, 10], './data/en-pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(sys_filename: str, refs_filename: str):\n",
    "    with open(sys_filename, 'r') as sys_file, \\\n",
    "         open(refs_filename, 'r') as refs_file:\n",
    "        sys = sys_file.readlines()\n",
    "        refs = refs_file.readlines()\n",
    "        \n",
    "        bleu = sacrebleu.BLEU()\n",
    "        score = bleu.corpus_score(sys, [refs])\n",
    "        \n",
    "        # print(bleu.get_signature())\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 23.82 47.6/28.1/19.5/14.0 (BP = 0.970 ratio = 0.970 hyp_len = 19877 ref_len = 20490)\n",
      "BLEU = 41.14 57.9/43.7/36.7/31.7 (BP = 0.993 ratio = 0.993 hyp_len = 20345 ref_len = 20490)\n",
      "BLEU = 42.98 58.9/45.3/38.4/33.4 (BP = 1.000 ratio = 1.000 hyp_len = 20488 ref_len = 20490)\n",
      "BLEU = 43.17 59.4/45.8/38.9/34.0 (BP = 0.991 ratio = 0.991 hyp_len = 20305 ref_len = 20490)\n"
     ]
    }
   ],
   "source": [
    "# From EN to PL\n",
    "print(evaluate_bleu(sys_filename='./data/en-pl/00.txt', refs_filename='./data/val.pl.txt'))\n",
    "print(evaluate_bleu(sys_filename='./data/en-pl/02.txt', refs_filename='./data/val.pl.txt'))\n",
    "print(evaluate_bleu(sys_filename='./data/en-pl/05.txt', refs_filename='./data/val.pl.txt'))\n",
    "print(evaluate_bleu(sys_filename='./data/en-pl/10.txt', refs_filename='./data/val.pl.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 29.08 54.1/32.9/23.2/17.3 (BP = 1.000 ratio = 1.025 hyp_len = 22949 ref_len = 22386)\n",
      "BLEU = 47.06 63.8/49.3/42.1/37.1 (BP = 1.000 ratio = 1.005 hyp_len = 22488 ref_len = 22386)\n",
      "BLEU = 48.28 64.6/50.5/43.4/38.3 (BP = 1.000 ratio = 1.005 hyp_len = 22503 ref_len = 22386)\n",
      "BLEU = 49.27 65.1/51.4/44.5/39.6 (BP = 1.000 ratio = 1.001 hyp_len = 22407 ref_len = 22386)\n"
     ]
    }
   ],
   "source": [
    "# From PL to EN\n",
    "print(evaluate_bleu(sys_filename='./data/pl-en/00.txt', refs_filename='./data/val.en.txt'))\n",
    "print(evaluate_bleu(sys_filename='./data/pl-en/02.txt', refs_filename='./data/val.en.txt'))\n",
    "print(evaluate_bleu(sys_filename='./data/pl-en/05.txt', refs_filename='./data/val.en.txt'))\n",
    "print(evaluate_bleu(sys_filename='./data/pl-en/10.txt', refs_filename='./data/val.en.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChfR Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chfr(sys_filename: str, refs_filename: str):\n",
    "    with open(sys_filename, 'r') as sys_file, \\\n",
    "         open(refs_filename, 'r') as refs_file:\n",
    "        sys = sys_file.readlines()\n",
    "        refs = refs_file.readlines()\n",
    "        \n",
    "        chrf = sacrebleu.CHRF()\n",
    "        score = chrf.corpus_score(sys, [refs])\n",
    "\n",
    "        # print(chrf.get_signature())\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrF2 = 47.50\n",
      "chrF2 = 58.54\n",
      "chrF2 = 59.60\n",
      "chrF2 = 59.79\n"
     ]
    }
   ],
   "source": [
    "# From EN to PL\n",
    "print(evaluate_chfr(sys_filename='./data/en-pl/00.txt', refs_filename='./data/val.pl.txt'))\n",
    "print(evaluate_chfr(sys_filename='./data/en-pl/02.txt', refs_filename='./data/val.pl.txt'))\n",
    "print(evaluate_chfr(sys_filename='./data/en-pl/05.txt', refs_filename='./data/val.pl.txt'))\n",
    "print(evaluate_chfr(sys_filename='./data/en-pl/10.txt', refs_filename='./data/val.pl.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrF2 = 52.60\n",
      "chrF2 = 62.75\n",
      "chrF2 = 63.78\n",
      "chrF2 = 64.13\n"
     ]
    }
   ],
   "source": [
    "# From PL to EN\n",
    "print(evaluate_chfr(sys_filename='./data/pl-en/00.txt', refs_filename='./data/val.en.txt'))\n",
    "print(evaluate_chfr(sys_filename='./data/pl-en/02.txt', refs_filename='./data/val.en.txt'))\n",
    "print(evaluate_chfr(sys_filename='./data/pl-en/05.txt', refs_filename='./data/val.en.txt'))\n",
    "print(evaluate_chfr(sys_filename='./data/pl-en/10.txt', refs_filename='./data/val.en.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TER Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ter(sys_filename: str, refs_filename: str):\n",
    "    with open(sys_filename, 'r') as sys_file, \\\n",
    "         open(refs_filename, 'r') as refs_file:\n",
    "        sys = sys_file.readlines()\n",
    "        refs = refs_file.readlines()\n",
    "\n",
    "        ter = sacrebleu.TER()\n",
    "        score = ter.corpus_score(sys, [refs])\n",
    "\n",
    "        # print(ter.get_signature())\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TER = 80.44\n",
      "TER = 65.48\n",
      "TER = 64.38\n",
      "TER = 63.64\n"
     ]
    }
   ],
   "source": [
    "# From EN to PL\n",
    "print(evaluate_ter(sys_filename='./data/en-pl/00.txt', refs_filename='./data/val.pl.txt'))\n",
    "print(evaluate_ter(sys_filename='./data/en-pl/02.txt', refs_filename='./data/val.pl.txt'))\n",
    "print(evaluate_ter(sys_filename='./data/en-pl/05.txt', refs_filename='./data/val.pl.txt'))\n",
    "print(evaluate_ter(sys_filename='./data/en-pl/10.txt', refs_filename='./data/val.pl.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TER = 74.86\n",
      "TER = 59.69\n",
      "TER = 57.99\n",
      "TER = 57.02\n"
     ]
    }
   ],
   "source": [
    "# From PL to EN\n",
    "print(evaluate_ter(sys_filename='./data/pl-en/00.txt', refs_filename='./data/val.en.txt'))\n",
    "print(evaluate_ter(sys_filename='./data/pl-en/02.txt', refs_filename='./data/val.en.txt'))\n",
    "print(evaluate_ter(sys_filename='./data/pl-en/05.txt', refs_filename='./data/val.en.txt'))\n",
    "print(evaluate_ter(sys_filename='./data/pl-en/10.txt', refs_filename='./data/val.en.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METEOR Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/piotr/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/piotr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/piotr/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "def evaluate_meteor(sys_filename: str, refs_filename: str):\n",
    "    with open(sys_filename, 'r') as sys_file, \\\n",
    "         open(refs_filename, 'r') as refs_file:\n",
    "        sys = sys_file.readlines()\n",
    "        refs = refs_file.readlines()\n",
    "\n",
    "    results = meteor.compute(predictions=[s.strip() for s in sys], references=[r.strip() for r in refs])\n",
    "    return results['meteor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35951175197195473\n",
      "0.49457376397913616\n",
      "0.5044096983347446\n",
      "0.5046950829450687\n"
     ]
    }
   ],
   "source": [
    "# From EN to PL\n",
    "print(evaluate_meteor(sys_filename='./data/en-pl/00.txt', refs_filename='./data/val.pl.txt'))\n",
    "print(evaluate_meteor(sys_filename='./data/en-pl/02.txt', refs_filename='./data/val.pl.txt'))\n",
    "print(evaluate_meteor(sys_filename='./data/en-pl/05.txt', refs_filename='./data/val.pl.txt'))\n",
    "print(evaluate_meteor(sys_filename='./data/en-pl/10.txt', refs_filename='./data/val.pl.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46228385555508894\n",
      "0.5632063254041779\n",
      "0.5735138695453181\n",
      "0.5763032255524402\n"
     ]
    }
   ],
   "source": [
    "# From PL to EN\n",
    "print(evaluate_meteor(sys_filename='./data/pl-en/00.txt', refs_filename='./data/val.en.txt'))\n",
    "print(evaluate_meteor(sys_filename='./data/pl-en/02.txt', refs_filename='./data/val.en.txt'))\n",
    "print(evaluate_meteor(sys_filename='./data/pl-en/05.txt', refs_filename='./data/val.en.txt'))\n",
    "print(evaluate_meteor(sys_filename='./data/pl-en/10.txt', refs_filename='./data/val.en.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMET Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "model = load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_comet(src_filename: str, sys_filename: str, refs_filename: str):\n",
    "    with open(src_filename, 'r') as src_file, \\\n",
    "         open(sys_filename, 'r') as sys_file, \\\n",
    "         open(refs_filename, 'r') as refs_file:\n",
    "        src = src_file.readlines()\n",
    "        sys = sys_file.readlines()\n",
    "        refs = refs_file.readlines()\n",
    "        data = [\n",
    "            {\"src\": src, \"mt\": mt, \"ref\": ref}\n",
    "            for src, mt, ref in zip(src, sys, refs)\n",
    "        ]\n",
    "\n",
    "        predictions = model.predict(data, batch_size=8, gpus=0)\n",
    "        return sum(predictions[\"scores\"]) / len(predictions[\"scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From EN to PL\n",
    "print(evaluate_comet(src_filename='./data/val.en.txt', refs_filename='./data/val.pl.txt', sys_filename='./data/en-pl/00.txt'))\n",
    "print(evaluate_comet(src_filename='./data/val.en.txt', refs_filename='./data/val.pl.txt', sys_filename='./data/en-pl/02.txt'))\n",
    "print(evaluate_comet(src_filename='./data/val.en.txt', refs_filename='./data/val.pl.txt', sys_filename='./data/en-pl/05.txt'))\n",
    "print(evaluate_comet(src_filename='./data/val.en.txt', refs_filename='./data/val.pl.txt', sys_filename='./data/en-pl/10.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From PL to EN\n",
    "print(evaluate_comet(src_filename='./data/val.pl.txt', refs_filename='./data/val.en.txt', sys_filename='./data/pl-en/00.txt'))\n",
    "print(evaluate_comet(src_filename='./data/val.pl.txt', refs_filename='./data/val.en.txt', sys_filename='./data/pl-en/02.txt'))\n",
    "print(evaluate_comet(src_filename='./data/val.pl.txt', refs_filename='./data/val.en.txt', sys_filename='./data/pl-en/05.txt'))\n",
    "print(evaluate_comet(src_filename='./data/val.pl.txt', refs_filename='./data/val.en.txt', sys_filename='./data/pl-en/10.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
