{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from rapidfuzz import process, fuzz, utils\n",
    "from typing import Dict, Literal, TypedDict\n",
    "import tqdm\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "\n",
    "def make_openai_api_call(context: str, prompt: str):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": context },\n",
    "            { \"role\": \"user\", \"content\": prompt }\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationExample(TypedDict):\n",
    "    text: str\n",
    "    line: str\n",
    "    confidence: float\n",
    "    translation: str\n",
    "\n",
    "def find_n_fuzzy_matches(\n",
    "        text: str, \n",
    "        source_filename: str, \n",
    "        n: int\n",
    "    ) -> list[tuple[str,float, int]]:\n",
    "    with open(source_filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        matches = process.extract(\n",
    "            query=text, \n",
    "            choices=lines, \n",
    "            scorer=fuzz.ratio, \n",
    "            limit=n, \n",
    "            processor=utils.default_process\n",
    "        )\n",
    "        \n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line_n(filename: str, n: int):\n",
    "    content = subprocess.run(\n",
    "        args=['sed', '-n', f'{n}p', filename], \n",
    "        capture_output=True, \n",
    "        text=True\n",
    "    )\n",
    "    return content.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind_fuzzy_matches_with_translations(\n",
    "    fuzzy_matches: list[tuple[str,float, int]], \n",
    "    target_filename: str\n",
    "    ) -> list[TranslationExample]:\n",
    "    result = []\n",
    "    for text, score, line in fuzzy_matches:\n",
    "        result.append(TranslationExample({\n",
    "            \"text\": text.strip(),\n",
    "            \"score\": round(score, 2),\n",
    "            \"translation\": read_line_n(\n",
    "                filename=target_filename, \n",
    "                n=line + 1\n",
    "            ).strip()\n",
    "        }))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Language = Literal[\"polski\",\"angielski\"]\n",
    "TEMP_FILENAME = \"./data/temp.txt\"\n",
    "context = \"Jesteś pomocnym bilingwalnym tłumaczem specjalizującym się w tłumaczeniach pomiędzy językiem polskim, a angielskim. Jako wynik zwracasz samo tłumaczenie.\"\n",
    "\n",
    "lang_to_filename: Dict[Language, str] = {\n",
    "    \"angielski\": \"./data/train.en.txt\",\n",
    "    \"polski\" : \"./data/train.pl.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(\n",
    "    text: str,\n",
    "    source_language: Language,\n",
    "    target_language: Language,\n",
    "    n_shots: int):\n",
    "    source_filename = lang_to_filename[source_language]\n",
    "    target_filename = lang_to_filename[target_language]\n",
    "\n",
    "    matches = find_n_fuzzy_matches(\n",
    "        text=text, \n",
    "        source_filename=source_filename, \n",
    "        n=n_shots\n",
    "    )\n",
    "\n",
    "    matches_with_translations = bind_fuzzy_matches_with_translations(\n",
    "        fuzzy_matches=matches,\n",
    "        target_filename=target_filename\n",
    "    )\n",
    "   \n",
    "    def create_shot(match: TranslationExample):\n",
    "        return f\"{source_language}: {match['text']}\\n\" + \\\n",
    "               f\"{target_language}: {match['translation']}\"\n",
    "\n",
    "    prompt = \\\n",
    "        f\"Przetłumacz zdania z języka {source_language}ego \" + \\\n",
    "        f\"na język {target_language}, biorąc pod uwagę \" + \\\n",
    "        f\"przykłady tłumaczeń zdań przybliżonych.\\n\" + \\\n",
    "        \"\\n\".join([create_shot(m) for m in matches_with_translations]) + \\\n",
    "        f\"\\n{source_language}: {text}\" + \\\n",
    "        f\"\\n{target_language}: \"\n",
    "    translation = make_openai_api_call(context, prompt)\n",
    "    \n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(text: str, source_language: Language, target_language: Language, n_shots: list[int]):\n",
    "    translations = []\n",
    "\n",
    "    if max(n_shots) > 1:\n",
    "        matches = find_n_fuzzy_matches(\n",
    "            text=text, \n",
    "            source_filename=lang_to_filename[source_language], \n",
    "            target_filename=lang_to_filename[target_language], \n",
    "            n=max(n_shots)\n",
    "        )\n",
    "    \n",
    "    for n in n_shots:\n",
    "        if n == 0:\n",
    "            prompt = f\"Przetłumacz z języka {source_language}ego na język {target_language}.\\n\" + \\\n",
    "                    f\"{source_language}: {text}\\n\" + \\\n",
    "                    f\"{target_language}:\"\n",
    "            t = make_openai_api_call(context, prompt)\n",
    "            translations.append(t)\n",
    "        else:\n",
    "            def create_shot(match: str):\n",
    "                return f\"{source_language}: {match.get('text')}\\n\" + \\\n",
    "                    f\"{target_language}: {match.get('translation')}\"\n",
    "\n",
    "            prompt = f\"Przetłumacz zdania z języka {source_language}ego\" + \\\n",
    "                    f\"na język {target_language}, biorąc pod uwagę \" + \\\n",
    "                    f\"przykłady tłumaczeń zdań podobnych.\\n\" + \\\n",
    "                    \"\\n\".join([create_shot(m) for m in matches[:n]]) + \\\n",
    "                    f\"\\n{source_language}: {text}\" + \\\n",
    "                    f\"\\n{target_language}: \"\n",
    "            t = make_openai_api_call(context, prompt)\n",
    "            translations.append(t)\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_lang_to_filename: Dict[Language, str] = {\n",
    "    \"angielski\": './data/val.en.txt',\n",
    "    \"polski\" : './data/val.pl.txt'\n",
    "}\n",
    "\n",
    "def ngram_translations(source_language: Language, target_langauge: Language, ngrams: list[str], target_folder: str):\n",
    "    source_file = val_lang_to_filename[source_language]\n",
    "\n",
    "    with open(source_file, \"r\") as file,\\\n",
    "         open(f\"{target_folder}/00.txt\", 'a') as file_res_zero,\\\n",
    "         open(f\"{target_folder}/02.txt\", 'a') as file_res_two,\\\n",
    "         open(f\"{target_folder}/05.txt\", 'a') as file_res_five,\\\n",
    "         open(f\"{target_folder}/10.txt\", 'a') as file_res_ten:\n",
    "        \n",
    "        for sentence in tqdm(file):\n",
    "            sentence = sentence.strip()\n",
    "            translations = translate_batch(sentence, source_language, target_langauge, ngrams)\n",
    "\n",
    "            file_res_zero.write(str(translations[0]).strip() + '\\n')\n",
    "            file_res_two.write(str(translations[1]).strip() + '\\n')\n",
    "            file_res_five.write(str(translations[2]).strip() + '\\n')\n",
    "            file_res_ten.write(str(translations[3]).strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [3:59:50, 14.39s/it]\n"
     ]
    }
   ],
   "source": [
    "ngram_translations('angielski', 'polski', [0, 2, 5, 10], './data/en-pl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
