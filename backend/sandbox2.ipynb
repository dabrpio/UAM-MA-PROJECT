{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames\n",
    "file_small_1 = 'data/data-en.txt'\n",
    "file_small_2 = 'data/data-pl.txt'\n",
    "file_large_1 = 'data/data-en-large.txt'\n",
    "file_large_2 = 'data/data-pl-large.txt'\n",
    "file_emea_1 = 'data/EMEA.en-pl.en'\n",
    "file_emea_2 = 'data/EMEA.en-pl.pl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-e 'Benefits payable' -e 'payable from' -e 'from the' -e 'the social' -e 'social insurance' -e 'insurance system' -e 'system are' -e 'are financed' -e 'financed by' -e 'by the' -e 'the Social' -e 'Social Insurance' -e 'Insurance Fund'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a smaller file with sentences partially including source text.\n",
    "import os, re\n",
    "from nltk import bigrams\n",
    "\n",
    "temp_file = 'data/temp.txt'\n",
    "example_text = \"Benefits payable from the social insurance system are financed by the Social Insurance Fund\"\n",
    "\n",
    "regex = re.compile('[^a-zA-Z\\s]')\n",
    "example_text_words = regex.sub('', example_text).split()\n",
    "if len(example_text_words) > 1:\n",
    "    words = list(bigrams(example_text_words))\n",
    "    patterns = ' '.join(f\"-e '{word[0]} {word[1]}'\" for word in words)\n",
    "else: \n",
    "    patterns = f\"-e '{example_text}'\"\n",
    "print(patterns)\n",
    "# os.system(\"echo %s | xargs -n1 -I{} grep -n -F {} input_file.txt > output_file.txt\")\n",
    "os.system(\"grep -n -w -i -F %s %s | cat >> %s\" %(patterns, file_large_1, temp_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in temp_file:\n",
      "133660 data/temp.txt\n",
      "\n",
      "\n",
      "First few lines from temp_file:\n",
      "400:You must be hungry, dear.\n",
      "1064:You must be crazy!\n",
      "1134:-He must be sensitive.\n",
      "1772:It must be 'that day' for Eunchae.\n",
      "1841:Instead of wearing yourself out about it, It's better to not start something that isn't meant to happen.\n",
      "2007:You must be the new guy that moved in downstairs.\n",
      "2532:anyway that girl must be crazy!\n",
      "2620:Come on...teacher must be waiting.\n",
      "3062:You must be stegnesh\n",
      "3074:I must be immune.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of lines in temp_file:')\n",
    "os.system(\"wc -l %s\" % temp_file)\n",
    "print('\\n')\n",
    "print('First few lines from temp_file:')\n",
    "os.system(\"head %s\" % temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Original Name                              Lookup 1  Lookup 1 Confidence  Lookup 1 Index                                             Lookup 2  Lookup 2 Confidence  Lookup 2 Index                                    Lookup 3  Lookup 3 Confidence  Lookup 3 Index                             Lookup 4  Lookup 4 Confidence  Lookup 4 Index                                                                       Lookup 5  Lookup 5 Confidence  Lookup 5 Index\n",
      "0  I must be leaving now.  3498566:I must be leaving you now.\\n                 0.64           22527  21481679:I'm leaving now. You're leaving with me.\\n                 0.63           71945  2071944:I must be leaving now, my dears.\\n                 0.62           43567  6437504:- Now. We're leaving now.\\n                 0.59           71309  4268527:Mr. Hastings will be leaving now. Mr. Hastings will be leaving now.\\n                 0.58           84496\n"
     ]
    }
   ],
   "source": [
    "# Utilize tfidf_matcher library to perform fuzzy matching using cosine similarity\n",
    "import tfidf_matcher as tm\n",
    "\n",
    "with open(temp_file, 'r', encoding='utf-8') as f1:\n",
    "    lines = f1.readlines()\n",
    "    unique_lines = set(lines)\n",
    "    result = tm.matcher(original=[example_text], lookup=unique_lines, k_matches=5, ngram_length=2)\n",
    "    print(result.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Command 'sed -n 10p a' returned non-zero exit status 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sed: can't read a: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# from itertools import islice\n",
    "\n",
    "# def read_line_n(filename, n):\n",
    "#     with open(filename, 'r') as file:\n",
    "#         line = next(islice(file, n-1, n), None)\n",
    "#     return line\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def read_line_n(filename, n):\n",
    "    command = f\"sed -n {n}p {filename}\"\n",
    "    try:\n",
    "        result = subprocess.check_output(command, shell=True, text=True)\n",
    "        return result.strip()\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "read_line_n('a', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16878342628479004\n",
      "            Original Name        Lookup 1  Lookup 1 Confidence  Lookup 1 Index      Lookup 2  Lookup 2 Confidence  Lookup 2 Index            Lookup 3  Lookup 3 Confidence  Lookup 3 Index              Lookup 4  Lookup 4 Confidence  Lookup 4 Index           Lookup 5  Lookup 5 Confidence  Lookup 5 Index\n",
      "0  I must be leaving now.  - Leaving now.                 0.88           44903  Leaving now.                 0.84            1549  I must be leaving.                 0.79           18463  I'll be leaving now.                 0.79           17982  're leaving now .                 0.79           59193\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "data_dict = {}\n",
    "k_matches = 5\n",
    "with open(temp_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    start = time.time()\n",
    "    unique_lines = set(lines)\n",
    "    for line in unique_lines:\n",
    "        number, text = line.strip().split(':', 1)\n",
    "        data_dict[text] = int(number)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    result = tm.matcher(original=[example_text], lookup=data_dict.keys(), k_matches=k_matches, ngram_length=4)\n",
    "    print(result.to_string())\n",
    "\n",
    "    res = []\n",
    "    for i in range(k_matches):\n",
    "        text = result[f'Lookup {i + 1}'].item()\n",
    "        # print(text)\n",
    "        # print(data_dict.get(text))\n",
    "        # print(read_line_n(file_large_2, data_dict.get(text)))\n",
    "        res.append({\n",
    "            'line': data_dict.get(text),\n",
    "            'source_text': text,\n",
    "            'target_text': read_line_n(file_large_2, data_dict.get(text)),\n",
    "            'probability': result[f'Lookup {i + 1} Confidence'].item()\n",
    "        })\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'line': 33538885, 'source_text': '- Leaving now.', 'target_text': 'Idziemy.', 'probability': 0.88}, {'line': 35292179, 'source_text': 'Leaving now.', 'target_text': 'Już wyruszam.', 'probability': 0.84}, {'line': 770694, 'source_text': 'I must be leaving.', 'target_text': 'Muszę już iść.', 'probability': 0.79}, {'line': 11342999, 'source_text': \"I'll be leaving now.\", 'target_text': 'Pójdę już.', 'probability': 0.79}, {'line': 33384470, 'source_text': \"'re leaving now .\", 'target_text': 'odejć teraz.', 'probability': 0.79}]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "temp_file = 'data/temp.txt'\n",
    "os.remove(temp_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
