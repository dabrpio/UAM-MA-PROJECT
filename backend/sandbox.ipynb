{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "\n",
    "def create_ngrams(words: list[str], n: int) -> list[str]:\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = words[i:i+n]\n",
    "        ngram_string = ' '.join(ngram)\n",
    "        ngrams.append(ngram_string)\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "def parse_sentence_to_ngrams(sentence: str) -> list[str]:\n",
    "    words = re.sub(r'[^\\p{L}\\s]', '', sentence).split()\n",
    "    words_count = len(words)\n",
    "\n",
    "    if words_count > 20:\n",
    "        return create_ngrams(words, 5)\n",
    "    if words_count > 14:\n",
    "        return create_ngrams(words, 4)\n",
    "    if words_count > 8:\n",
    "        return create_ngrams(words, 3)\n",
    "    if words_count > 2:\n",
    "        return create_ngrams(words, 2)\n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence_to_ngrams_2(sentence: str, n: int) -> list[str]:\n",
    "    words = re.sub(r'[^\\p{L}\\s]', '', sentence).split()\n",
    "\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = words[i:i+n]\n",
    "        ngram_string = ' '.join(ngram)\n",
    "        ngrams.append(ngram_string)\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def save_texts_with_ngrams_in_file(ngrams: list[str], source_filename: str, temp_filename: str):\n",
    "    try:\n",
    "        args = ['grep', '--fixed-strings', '--ignore-case', '--line-number']\n",
    "        for ngram in ngrams:\n",
    "            args.extend(['-e', ngram])\n",
    "        args.append(source_filename)\n",
    "\n",
    "        with open(temp_filename, 'w', encoding='utf-8') as f:\n",
    "            subprocess.run(\n",
    "                args=args,\n",
    "                stdout=f,\n",
    "                stderr=subprocess.PIPE,\n",
    "                text=True,\n",
    "            )\n",
    " \n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # Obsługa błędów, gdy grep zwróci niezerowy kod wyjścia\n",
    "        print(f\"Błąd: Komenda zakończyła się niepowodzeniem: {e}\")\n",
    "        print(f\"Błędy: {e}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        # Obsługa przypadku, gdy plik lub komenda nie zostanie znaleziona\n",
    "        print(f\"Nie znaleziono pliku lub komendy: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Ogólna obsługa innych błędów\n",
    "        print(f\"Nieoczekiwany błąd: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('uwzględniając wspólny wniosek Komisji Europejskiej i Wysokiego Przedstawiciela Unii do Spraw Zagranicznych i Polityki Bezpieczeństwa ,\\n', 98.87640449438202, 147617), ('uwzględniając wspólny wniosek Komisji Europejskiej i Wysokiego Przedstawiciela Unii do Spraw Zagranicznych i Polityki Bezpieczeństwa ,\\n', 98.87640449438202, 1605432)]\n"
     ]
    }
   ],
   "source": [
    "# sentence = \"\"\n",
    "\n",
    "# n = 1\n",
    "# line_count = \n",
    "# while \n",
    "# parse_sentence_to_ngrams_2(sentence, )\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "with open(\"./data/train.pl.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    sentence = \"uwzględniając wspólny wniosek Komisji Europejskiej i Wysokiego Przedstawiciela Unii do Spraw Zagranicznych i Polityki Bezpieczeństwa\"\n",
    "    res = process.extract(sentence, lines, scorer=fuzz.WRatio, limit=2)\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line_n(filename: str, n: int):\n",
    "    with open(filename, 'r') as f:\n",
    "        content = subprocess.run(args=['sed', '-n', f'{n}p', filename], capture_output=True, text=True)\n",
    "        return content.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "def create_model(\n",
    "        src_filename: str,\n",
    "        k_neighbors = 5, \n",
    "        n_grams = 3\n",
    "    ):\n",
    "    with open(src_filename, 'r', encoding='utf-8') as f:\n",
    "        text_list = []\n",
    "        file_lines = f.readlines()\n",
    "        for line in file_lines:\n",
    "            text_list.append(line.strip())\n",
    "\n",
    "        vectorizer = TfidfVectorizer(min_df=1, analyzer='word')\n",
    "        tfidf_matrix = vectorizer.fit_transform(text_list)\n",
    "\n",
    "        nbrs_model = NearestNeighbors(\n",
    "            n_neighbors=k_neighbors, \n",
    "            n_jobs=-2, \n",
    "            metric=\"cosine\"\n",
    "        ).fit(tfidf_matrix)\n",
    "\n",
    "        return nbrs_model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vectorizer = create_model('./data/train.en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "\n",
    "class SentenceMatch(TypedDict):\n",
    "    text: str\n",
    "    translation: str\n",
    "    confidence: float\n",
    "\n",
    "def predict_fuzzy_matches(\n",
    "        sentence: str,\n",
    "        src_filename: str, \n",
    "        translation_filename: str, \n",
    "        ) -> List[SentenceMatch]:\n",
    "    with open(src_filename, 'r', encoding='utf-8') as f:\n",
    "        text_list = []\n",
    "        file_lines = f.readlines()\n",
    "        for line in file_lines:\n",
    "            text_list.append(line.strip())\n",
    "\n",
    "\n",
    "        tfidf_sentence = vectorizer.transform([sentence])\n",
    "        distances, positions = model.kneighbors(tfidf_sentence)\n",
    "\n",
    "        confidences = [round(1 - dist, 2) for dist in distances[0]]\n",
    "        result = []\n",
    "        for i, position in enumerate(positions[0]):\n",
    "            text = text_list[position]\n",
    "            confidence = confidences[i]\n",
    "            translation = read_line_n(translation_filename, position + 1)\n",
    "\n",
    "            result.append(SentenceMatch({\n",
    "                'text': text,\n",
    "                'translation': translation,\n",
    "                'confidence': confidence\n",
    "            }))\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = predict_fuzzy_matches(\"A number of amendments to the international conventions and\", './data/train.en.txt', './data/train.pl.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'apply , for the purpose of this Directive , amendments made to the International Conventions ;', 'translation': 'stosowania do celów niniejszej dyrektywy zmian dokonanych w Międzynarodowych Konwencjach ;\\n', 'confidence': 0.7}, {'text': 'CONVENTIONS', 'translation': 'W stosownych przypadkach na podstawie krajowych ogólnie przyjętych zasad rachunkowości opartych na BAD zgłasza się » Redukcje wartości w odniesieniu do pozycji przeznaczonych do obrotu wycenianych według wartości godziwej « .\\n', 'confidence': 0.64}, {'text': 'CONVENTIONS', 'translation': 'Zmniejszenie zobowiązań\\n', 'confidence': 0.64}, {'text': 'Conventions', 'translation': 'Konwencje\\n', 'confidence': 0.64}, {'text': 'Conventions', 'translation': 'Zmiany w odpisach aktualizujących z tytułu strat kredytowych i utraty wartości instrumentów kapitałowych na podstawie krajowych ogólnie przyjętych zasad rachunkowości opartych na BAD ( 12.0 )\\n', 'confidence': 0.64}]\n"
     ]
    }
   ],
   "source": [
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from typing import TypedDict, List\n",
    "\n",
    "class SentenceMatch(TypedDict):\n",
    "    text: str\n",
    "    translation: str\n",
    "    confidence: float\n",
    "\n",
    "def find_fuzzy_matches(\n",
    "        sentence: str, \n",
    "        temp_filename: str , \n",
    "        translation_filename: str, \n",
    "        k_neighbors = 5, \n",
    "        n_grams = 3) -> List[SentenceMatch]:\n",
    "    with open(temp_filename, 'r', encoding='utf-8') as f:\n",
    "        line_number_list = []\n",
    "        text_list = []\n",
    "\n",
    "        file_lines = f.readlines()\n",
    "        for line in file_lines:\n",
    "            line_number, text = line.strip().split(':', 1)\n",
    "            line_number_list.append(line_number)\n",
    "            text_list.append(text)\n",
    "\n",
    "        def analyzer(string: str, n = n_grams):\n",
    "            string = re.sub(r'[^\\p{L}\\s]', r'', string)\n",
    "            ngrams = zip(*[string[i:] for i in range(n)])\n",
    "            return [\"\".join(ngram) for ngram in ngrams]\n",
    "\n",
    "        vectorizer = TfidfVectorizer(min_df=1, analyzer=analyzer)\n",
    "        tfidf_matrix = vectorizer.fit_transform(text_list)\n",
    "\n",
    "        nbrs_model = NearestNeighbors(\n",
    "            n_neighbors=k_neighbors, \n",
    "            n_jobs=-2, \n",
    "            metric=\"cosine\"\n",
    "        ).fit(tfidf_matrix)\n",
    "\n",
    "        tfidf_sentence = vectorizer.transform([sentence])\n",
    "        distances, positions = nbrs_model.kneighbors(tfidf_sentence)\n",
    "\n",
    "        confidences = [round(1 - dist, 2) for dist in distances[0]]\n",
    "        result = []\n",
    "        for i, position in enumerate(positions[0]):\n",
    "            text = text_list[position]\n",
    "            line_number = int(line_number_list[position])\n",
    "            confidence = confidences[i]\n",
    "            translation = read_line_n(translation_filename, line_number)\n",
    "\n",
    "            result.append(SentenceMatch({\n",
    "                'text': text,\n",
    "                'translation': translation,\n",
    "                'confidence': confidence\n",
    "            }))\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'el',\n",
       " 'll',\n",
       " 'lo',\n",
       " 'o ',\n",
       " ' m',\n",
       " 'my',\n",
       " 'y ',\n",
       " ' n',\n",
       " 'na',\n",
       " 'am',\n",
       " 'me',\n",
       " 'e ',\n",
       " ' i',\n",
       " 'is']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "def analyzer(string: str, n: int):\n",
    "    string = re.sub(r'[^\\p{L}\\s]', r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [\"\".join(ngram) for ngram in ngrams]\n",
    "\n",
    "analyzer(\"hello my name is\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Operator SIPS ustala i podaje do wiadomości publicznej niedyskryminujące procedury w zakresie zawieszenia uczestnictwa i uporządkowanego wypowiedzenia uczestnictwa w przypadku niespełniania przez danego uczestnika kryteriów dostępu i uczestnictwa .',\n",
       "  'translation': 'It shall review the procedures at least annually .',\n",
       "  'confidence': 0.68},\n",
       " {'text': 'podawanie do wiadomości publicznej :',\n",
       "  'translation': 'to make publicly available the following :',\n",
       "  'confidence': 0.43},\n",
       " {'text': 'dana instytucja zdrowia publicznego sporządza podawane do wiadomości publicznej oświadczenie zawierające :',\n",
       "  'translation': 'the health institution draws up a declaration which it shall make publicly available , including :',\n",
       "  'confidence': 0.38},\n",
       " {'text': 'Streszczenie sprawozdania rocznego podaje się do wiadomości publicznej . ” ;',\n",
       "  'translation': 'internal and external audit findings and the follow-up to the audit recommendations and to the discharge recommendation ;',\n",
       "  'confidence': 0.37},\n",
       " {'text': 'w sposób niedyskryminujący ; oraz',\n",
       "  'translation': 'In order to ascertain whether the measure breaches this Article , the Tribunal must consider whether a Party has acted inconsistently with the obligations in paragraph 1 .',\n",
       "  'confidence': 0.37}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_fuzzy_matches(\"wiadomości publicznej niedyskryminujące procedury w zakresie zawieszenia uczestnictwa \", './data/temp2.txt', './data/train.en.txt', 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "\n",
    "def make_openai_api_call(context: str, prompt: str):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": context },\n",
    "            { \"role\": \"user\", \"content\": prompt }\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_length(filename: str):\n",
    "    with open(file=filename, encoding='utf-8', mode='r') as f:\n",
    "        lines = f.readlines()\n",
    "        return len(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Literal\n",
    "\n",
    "\n",
    "Language = Literal[\"polski\",\"angielski\"]\n",
    "TEMP_FILENAME = \"./data/temp.txt\"\n",
    "context = \"Jesteś pomocnym bilingwalnym tłumaczem specjalizującym się w tłumaczeniach pomiędzy językiem polskim, a angielskim. Jako wynik zwracasz samo tłumaczenie.\"\n",
    "\n",
    "dataset_filenames: Dict[Language, str] = {\n",
    "    \"angielski\": \"./data/train.en.txt\",\n",
    "    \"polski\" : \"./data/train.pl.txt\"\n",
    "}\n",
    "\n",
    "def translate(text: str, source_language: Language, target_language: Language, few_shots: int):\n",
    "    if few_shots == 0:\n",
    "        prompt = f\"Przetłumacz z języka {source_language}ego na język {target_language}.\\n\" + \\\n",
    "                 f\"{source_language}: {text}\\n\" + \\\n",
    "                 f\"{target_language}:\"\n",
    "        return make_openai_api_call(context, prompt)\n",
    "    else:\n",
    "        ngrams = parse_sentence_to_ngrams(text)\n",
    "\n",
    "        save_texts_with_ngrams_in_file(\n",
    "            ngrams=ngrams, \n",
    "            source_filename=dataset_filenames[source_language], \n",
    "            temp_filename=TEMP_FILENAME\n",
    "        )\n",
    "\n",
    "        # Return None if number of results in temp file is not enough \n",
    "        file_len = check_file_length(TEMP_FILENAME)\n",
    "        if file_len < few_shots:\n",
    "            return None\n",
    "\n",
    "        matches = find_fuzzy_matches(\n",
    "            sentence=text, \n",
    "            temp_filename=TEMP_FILENAME, \n",
    "            translation_filename=dataset_filenames[target_language], \n",
    "            k_neighbors=few_shots, \n",
    "            n_grams=3\n",
    "        )\n",
    "\n",
    "        def create_shot(match: str):\n",
    "            return f\"{source_language}: {match.get('text')}\\n\" + \\\n",
    "                   f\"{target_language}: {match.get('translation')}\"\n",
    "\n",
    "        prompt = f\"Przetłumacz zdania z języka {source_language}ego\" + \\\n",
    "                 f\"na język {target_language}, biorąc pod uwagę \" + \\\n",
    "                 f\"przykłady tłumaczeń zdań podobnych.\\n\" + \\\n",
    "                 \"\\n\".join([create_shot(m) for m in matches]) + \\\n",
    "                 f\"\\n{source_language}: {text}\" + \\\n",
    "                 f\"\\n{target_language}: \"\n",
    "        return make_openai_api_call(context, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Literal\n",
    "\n",
    "\n",
    "Language = Literal[\"polski\",\"angielski\"]\n",
    "TEMP_FILENAME = \"./data/temp.txt\"\n",
    "context = \"Jesteś pomocnym bilingwalnym tłumaczem specjalizującym się w tłumaczeniach pomiędzy językiem polskim, a angielskim. Jako wynik zwracasz samo tłumaczenie.\"\n",
    "\n",
    "dataset_filenames: Dict[Language, str] = {\n",
    "    \"angielski\": \"./data/train.en.txt\",\n",
    "    \"polski\" : \"./data/train.pl.txt\"\n",
    "}\n",
    "\n",
    "def translate(text: str, source_language: Language, target_language: Language, few_shots: int):\n",
    "    if few_shots == 0:\n",
    "        prompt = f\"Przetłumacz z języka {source_language}ego na język {target_language}.\\n\" + \\\n",
    "                 f\"{source_language}: {text}\\n\" + \\\n",
    "                 f\"{target_language}:\"\n",
    "        return make_openai_api_call(context, prompt)\n",
    "    else:\n",
    "        ngrams = parse_sentence_to_ngrams(text)\n",
    "\n",
    "        save_texts_with_ngrams_in_file(\n",
    "            ngrams=ngrams, \n",
    "            source_filename=dataset_filenames[source_language], \n",
    "            temp_filename=TEMP_FILENAME\n",
    "        )\n",
    "\n",
    "        # Return None if number of results in temp file is not enough \n",
    "        file_len = check_file_length(TEMP_FILENAME)\n",
    "        if file_len < few_shots:\n",
    "            return None\n",
    "\n",
    "        matches = find_fuzzy_matches(\n",
    "            sentence=text, \n",
    "            temp_filename=TEMP_FILENAME, \n",
    "            translation_filename=dataset_filenames[target_language], \n",
    "            k_neighbors=few_shots, \n",
    "            n_grams=3\n",
    "        )\n",
    "\n",
    "        def create_shot(match: str):\n",
    "            return f\"{source_language}: {match.get('text')}\\n\" + \\\n",
    "                   f\"{target_language}: {match.get('translation')}\"\n",
    "\n",
    "        prompt = f\"Przetłumacz zdania z języka {source_language}ego\" + \\\n",
    "                 f\"na język {target_language}, biorąc pod uwagę \" + \\\n",
    "                 f\"przykłady tłumaczeń zdań podobnych.\\n\" + \\\n",
    "                 \"\\n\".join([create_shot(m) for m in matches]) + \\\n",
    "                 f\"\\n{source_language}: {text}\" + \\\n",
    "                 f\"\\n{target_language}: \"\n",
    "        return make_openai_api_call(context, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Literal\n",
    "\n",
    "\n",
    "Language = Literal[\"polski\",\"angielski\"]\n",
    "TEMP_FILENAME = \"./data/temp.txt\"\n",
    "context = \"Jesteś pomocnym bilingwalnym tłumaczem specjalizującym się w tłumaczeniach pomiędzy językiem polskim, a angielskim. Jako wynik zwracasz samo tłumaczenie.\"\n",
    "\n",
    "dataset_filenames: Dict[Language, str] = {\n",
    "    \"angielski\": \"./data/train.en.txt\",\n",
    "    \"polski\" : \"./data/train.pl.txt\"\n",
    "}\n",
    "\n",
    "def translate_batch(text: str, source_language: Language, target_language: Language, n_shots: list[int]):\n",
    "    translations = []\n",
    "    \n",
    "    ngrams = parse_sentence_to_ngrams(text)\n",
    "    save_texts_with_ngrams_in_file(\n",
    "        ngrams=ngrams, \n",
    "        source_filename=dataset_filenames[source_language], \n",
    "        temp_filename=TEMP_FILENAME\n",
    "    )\n",
    "\n",
    "    file_len = check_file_length(TEMP_FILENAME)\n",
    "    matches = []\n",
    "    if file_len != 0:\n",
    "        matches = find_fuzzy_matches(\n",
    "            sentence=text, \n",
    "            temp_filename=TEMP_FILENAME, \n",
    "            translation_filename=dataset_filenames[target_language], \n",
    "            k_neighbors=min([max(n_shots), file_len]), \n",
    "            n_grams=3\n",
    "        )\n",
    "    \n",
    "    for n in n_shots:\n",
    "        if n_shots == 0:\n",
    "            prompt = f\"Przetłumacz z języka {source_language}ego na język {target_language}.\\n\" + \\\n",
    "                    f\"{source_language}: {text}\\n\" + \\\n",
    "                    f\"{target_language}:\"\n",
    "            t = make_openai_api_call(context, prompt)\n",
    "            translations.append(t)\n",
    "        else:\n",
    "            if len(matches) < n:\n",
    "                translations.append('None')\n",
    "                continue\n",
    "            \n",
    "            def create_shot(match: str):\n",
    "                return f\"{source_language}: {match.get('text')}\\n\" + \\\n",
    "                    f\"{target_language}: {match.get('translation')}\"\n",
    "\n",
    "            prompt = f\"Przetłumacz zdania z języka {source_language}ego\" + \\\n",
    "                    f\"na język {target_language}, biorąc pod uwagę \" + \\\n",
    "                    f\"przykłady tłumaczeń zdań podobnych.\\n\" + \\\n",
    "                    \"\\n\".join([create_shot(m) for m in matches]) + \\\n",
    "                    f\"\\n{source_language}: {text}\" + \\\n",
    "                    f\"\\n{target_language}: \"\n",
    "            t = make_openai_api_call(context, prompt)\n",
    "            translations.append(t)\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annex III , B.1\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Załącznik III , B.1\"\n",
    "\n",
    "\n",
    "translation = translate(sentence,  'polski', 'angielski', 10)\n",
    "print(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "val_en = './data/val.en.txt'\n",
    "val_pl = './data/val.pl.txt'\n",
    "val_res_zero = './data/10/00.txt'\n",
    "val_res_two = './data/10/02.txt'\n",
    "val_res_five = './data/10/05.txt'\n",
    "# val_res_ten = './data/new/10.txt'\n",
    "\n",
    " \n",
    "def ngram_translations(source_language: Language, target_langauge: Language, ngrams: list[str]):\n",
    "    source_file = val_en if source_language == 'angielski' else val_pl\n",
    "\n",
    "    with open(source_file, \"r\") as file,\\\n",
    "         open(val_res_zero, 'a') as file_res_zero,\\\n",
    "         open(val_res_two, 'a') as file_res_two,\\\n",
    "         open(val_res_five, 'a') as file_res_five:\n",
    "        #  open(val_res_ten, 'a') as file_res_ten:\n",
    "        for i, sentence in enumerate(tqdm(file)):\n",
    "            os.remove('data/temp.txt')\n",
    "            if i < 241:\n",
    "                continue\n",
    "            if i > 300:\n",
    "                break\n",
    "\n",
    "            sentence = sentence.strip()\n",
    "            translations = translate_batch(sentence, source_language, target_langauge, ngrams)\n",
    "            print(i, translations)\n",
    "\n",
    "            file_res_zero.write(str(translations[0]).strip() + '\\n')\n",
    "            file_res_two.write(str(translations[1]).strip() + '\\n')\n",
    "            file_res_five.write(str(translations[2]).strip() + '\\n')\n",
    "            # file_res_ten.write(str(translations[3]).strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ngram_translations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mngram_translations\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolski\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mangielski\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ngram_translations' is not defined"
     ]
    }
   ],
   "source": [
    "ngram_translations('polski', 'angielski', [0, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "model = load_from_checkpoint(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from typing import TypedDict, List\n",
    "\n",
    "class SentenceMatch(TypedDict):\n",
    "    text: str\n",
    "    translation: str\n",
    "    confidence: float\n",
    "\n",
    "def find_fuzzy_matches(\n",
    "        sentence: str, \n",
    "        temp_filename: str , \n",
    "        translation_filename: str, \n",
    "        k_neighbors = 5\n",
    "    ) -> List[SentenceMatch]:\n",
    "    with open(temp_filename, 'r', encoding='utf-8') as f:\n",
    "        file_lines = f.readlines()\n",
    "\n",
    "        vectorizer = TfidfVectorizer(min_df=1, analyzer='word')\n",
    "        tfidf_matrix = vectorizer.fit_transform(file_lines)\n",
    "\n",
    "        nbrs_model = NearestNeighbors(\n",
    "            n_neighbors=k_neighbors, \n",
    "            n_jobs=-1, \n",
    "            metric=\"cosine\"\n",
    "        ).fit(tfidf_matrix)\n",
    "\n",
    "        tfidf_sentence = vectorizer.transform([sentence])\n",
    "        distances, positions = nbrs_model.kneighbors(tfidf_sentence)\n",
    "\n",
    "        confidences = [round(1 - dist, 2) for dist in distances[0]]\n",
    "        result = []\n",
    "        for i, position in enumerate(positions[0]):\n",
    "            text = file_lines[position]\n",
    "            translation = read_line_n(translation_filename, position + 1)\n",
    "            confidence = confidences[i]\n",
    "            \n",
    "            result.append(SentenceMatch({\n",
    "                \"position\": position,\n",
    "                'text': text,\n",
    "                'translation': translation,\n",
    "                'confidence': confidence\n",
    "            }))\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = 'Wnioskodawca twierdził , że zakres produktu powinien zostać wyjaśniony w odniesieniu do zamknięć włazów z żeliwa sferoidalnego , a w szczególności należy wyjaśnić , czy ten rodzaj zamknięć włazów powinien mieścić się w zakresie definicji produktu objętego postępowaniem .'\n",
    "find_fuzzy_matches(sentence, './data/train.pl.txt', './data/train.en.txt', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Z treści pierwotnego zawiadomienia o wszczęciu nie wynika , że zamknięcia włazów wykonane z żeliwa sferoidalnego zostały wyraźnie lub domyślnie wyłączone z definicji produktu , którego dotyczy postępowanie .\\n', 54.43037974683544, 1884430)\n",
      "('Wnioskodawca twierdził również , że spółka dominująca nie powinna była zostać objęta zakresem dochodzenia dotyczącego MET , ponieważ nie jest ani producentem , ani eksporterem produktu objętego postępowaniem .\\n', 54.201680672268914, 477952)\n",
      "('Wnioskodawca twierdził , że gatunek do produkcji taśm powinien zostać uznany za niepodlegający zakresowi dochodzenia pierwotnego i , co za tym idzie , nie powinien być objęty wspomnianymi powyżej środkami .\\n', 53.276955602537, 110468)\n",
      "('Wnioskodawca twierdził , że wszelkie dostosowanie ceny gazu , jaką płaci on na rosyjskim rynku krajowym , będzie nieuzasadnione , ponieważ zapisy księgowe przedsiębiorstwa w pełni odzwierciedlają koszty związane z produkcją produktu objętego postępowaniem w Rosji .\\n', 53.00751879699248, 1593441)\n",
      "('Wnioskodawca twierdził , że podstawowe właściwości fizyczne , techniczne i chemiczne silikonowanej poliestrowej folii antyadhezyjnej różnią się od właściwości produktu objętego postępowaniem .\\n', 52.723311546840954, 2233179)\n",
      "('Stwierdził on również , że gatunek do produkcji taśm jest wykorzystywany do bardzo szczególnego celu przez przemysł motoryzacyjny i , co za tym idzie , nie powinien być uznawany za część produktu objętego postępowaniem .\\n', 52.56673511293635, 619973)\n",
      "('Jeden chiński producent eksportujący twierdził , że zakres produktu był zbyt szeroki , gdyż obejmuje on rodzaje produktu ze stali stopowej , których koszty i cena znacznie różnią się od kosztów i cen standardowego produktu objętego postępowaniem .\\n', 52.140077821011666, 367898)\n",
      "('ET Solar stwierdził również , że sprzedaż elektrowni słonecznych nie jest zakazana w ramach zobowiązania , ponieważ są to złożone , zintegrowane produkty , które należy traktować jako pojedynczą jednostkę i które jako takie nie wchodzą w zakres definicji produktu objętego zobowiązaniem .\\n', 51.891891891891895, 2020172)\n",
      "('Niektóre strony twierdziły , że aż do 65 % kategorii zwanej „ folią na rolce ” ( stanowiącej 15,3 % [ 7 ] całkowitego zastosowania końcowego ) wchodzi w zakres definicji produktu objętego dochodzeniem .\\n', 51.59914712153518, 76518)\n",
      "('Z powodów wskazanych w powyższych motywach 19 i 20 Komisja nie uważała , że należy wyłączyć rury z kołnierzami z zakresu definicji produktu objętego postępowaniem .\\n', 51.50812064965198, 2371797)\n"
     ]
    }
   ],
   "source": [
    "from rapidfuzz import process, fuzz, utils\n",
    "\n",
    "with open(\"./data/train.pl.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    sentence = 'Wnioskodawca twierdził , że zakres produktu powinien zostać wyjaśniony w odniesieniu do zamknięć włazów z żeliwa sferoidalnego , a w szczególności należy wyjaśnić , czy ten rodzaj zamknięć włazów powinien mieścić się w zakresie definicji produktu objętego postępowaniem .'\n",
    "    res = process.extract(sentence, lines, scorer=fuzz.QRatio, limit=10, processor=utils.default_process)\n",
    "    \n",
    "    for i in res:\n",
    "        print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
